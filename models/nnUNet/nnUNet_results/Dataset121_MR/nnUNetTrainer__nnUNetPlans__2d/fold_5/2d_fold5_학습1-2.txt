
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 14, 'patch_size': [448, 512], 'median_image_size_in_voxels': [392.0, 496.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [True], 'UNet_class_name': 'PlainConvUNet', 'UNet_base_num_features': 32, 'n_conv_per_stage_encoder': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'num_pool_per_axis': [6, 6], 'pool_op_kernel_sizes': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'unet_max_num_features': 512, 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset121_MR', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 392, 496], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 91.21957397460938, 'median': 78.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 255.0, 'std': 63.28199005126953}}} 
 
2023-11-30 09:52:10.606977: unpacking dataset... 
2023-11-30 09:52:12.196502: unpacking done... 
2023-11-30 09:52:12.201503: do_dummy_2d_data_aug: False 
2023-11-30 09:52:12.218516: Using splits from existing split file: C:/Users/qwe14/0.code/nnUNet/nnUNet_preprocessed\Dataset121_MR\splits_final.json 
2023-11-30 09:52:12.232534: The split file contains 5 splits. 
2023-11-30 09:52:12.237045: Desired fold for training: 5 
2023-11-30 09:52:12.243045: INFO: You requested fold 5 for training but splits contain only 5 folds. I am now creating a random (but seeded) 80:20 split! 
2023-11-30 09:52:12.250674: This random 80:20 split has 776 training and 194 validation cases. 
2023-11-30 09:52:12.332855: Unable to plot network architecture: 
2023-11-30 09:52:12.337448: No module named 'IPython' 
2023-11-30 09:52:12.369430:  
2023-11-30 09:52:12.374937: Epoch 100 
2023-11-30 09:52:12.379944: Current learning rate: 0.00267 
2023-11-30 09:54:01.343150: train_loss -0.96 
2023-11-30 09:54:01.366715: val_loss -0.9298 
2023-11-30 09:54:01.375757: Pseudo dice [0.982, 0.9833, 0.9559] 
2023-11-30 09:54:01.385274: Epoch time: 108.97 s 
2023-11-30 09:54:02.837961:  
2023-11-30 09:54:02.842924: Epoch 101 
2023-11-30 09:54:02.847883: Current learning rate: 0.00259 
2023-11-30 09:55:08.878463: train_loss -0.961 
2023-11-30 09:55:08.891493: val_loss -0.9291 
2023-11-30 09:55:08.901011: Pseudo dice [0.9825, 0.9831, 0.956] 
2023-11-30 09:55:08.923063: Epoch time: 66.04 s 
2023-11-30 09:55:10.372860:  
2023-11-30 09:55:10.378451: Epoch 102 
2023-11-30 09:55:10.383055: Current learning rate: 0.00251 
2023-11-30 09:56:19.998207: train_loss -0.9614 
2023-11-30 09:56:20.014239: val_loss -0.9284 
2023-11-30 09:56:20.029786: Pseudo dice [0.9825, 0.9833, 0.9561] 
2023-11-30 09:56:20.043331: Epoch time: 69.63 s 
2023-11-30 09:56:21.484621:  
2023-11-30 09:56:21.490263: Epoch 103 
2023-11-30 09:56:21.495307: Current learning rate: 0.00243 
2023-11-30 09:57:26.810053: train_loss -0.962 
2023-11-30 09:57:26.911239: val_loss -0.929 
2023-11-30 09:57:26.931274: Pseudo dice [0.9824, 0.9831, 0.9562] 
2023-11-30 09:57:26.941798: Epoch time: 65.33 s 
2023-11-30 09:57:28.242792:  
2023-11-30 09:57:28.250337: Epoch 104 
2023-11-30 09:57:28.254924: Current learning rate: 0.00235 
2023-11-30 09:58:33.815535: train_loss -0.9622 
2023-11-30 09:58:33.842075: val_loss -0.9257 
2023-11-30 09:58:33.867633: Pseudo dice [0.9823, 0.9829, 0.9545] 
2023-11-30 09:58:33.882662: Epoch time: 65.57 s 
2023-11-30 09:58:35.365567:  
2023-11-30 09:58:35.371703: Epoch 105 
2023-11-30 09:58:35.375700: Current learning rate: 0.00227 
2023-11-30 09:59:42.130412: train_loss -0.9621 
2023-11-30 09:59:42.139923: val_loss -0.9285 
2023-11-30 09:59:42.150452: Pseudo dice [0.9817, 0.9835, 0.9558] 
2023-11-30 09:59:42.164498: Epoch time: 66.77 s 
2023-11-30 09:59:43.567839:  
2023-11-30 09:59:43.574609: Epoch 106 
2023-11-30 09:59:43.579267: Current learning rate: 0.00219 
2023-11-30 10:00:50.247341: train_loss -0.9624 
2023-11-30 10:00:50.259368: val_loss -0.9273 
2023-11-30 10:00:50.269897: Pseudo dice [0.9821, 0.9833, 0.9559] 
2023-11-30 10:00:50.280420: Epoch time: 66.68 s 
2023-11-30 10:00:51.729240:  
2023-11-30 10:00:51.734210: Epoch 107 
2023-11-30 10:00:51.739173: Current learning rate: 0.0021 
2023-11-30 10:01:58.668591: train_loss -0.9631 
2023-11-30 10:01:58.699678: val_loss -0.926 
2023-11-30 10:01:58.719734: Pseudo dice [0.9827, 0.9828, 0.9549] 
2023-11-30 10:01:58.742783: Epoch time: 66.94 s 
2023-11-30 10:02:00.254787:  
2023-11-30 10:02:00.260107: Epoch 108 
2023-11-30 10:02:00.265105: Current learning rate: 0.00202 
2023-11-30 10:03:08.568380: train_loss -0.9632 
2023-11-30 10:03:08.586418: val_loss -0.9244 
2023-11-30 10:03:08.601454: Pseudo dice [0.9824, 0.983, 0.9551] 
2023-11-30 10:03:08.616971: Epoch time: 68.31 s 
2023-11-30 10:03:10.111422:  
2023-11-30 10:03:10.116947: Epoch 109 
2023-11-30 10:03:10.122162: Current learning rate: 0.00194 
2023-11-30 10:04:16.814828: train_loss -0.9631 
2023-11-30 10:04:16.828852: val_loss -0.926 
2023-11-30 10:04:16.841387: Pseudo dice [0.9824, 0.983, 0.9551] 
2023-11-30 10:04:16.860431: Epoch time: 66.7 s 
2023-11-30 10:04:18.359073:  
2023-11-30 10:04:18.365077: Epoch 110 
2023-11-30 10:04:18.369660: Current learning rate: 0.00186 
2023-11-30 10:05:24.628831: train_loss -0.9634 
2023-11-30 10:05:24.720973: val_loss -0.9246 
2023-11-30 10:05:24.856678: Pseudo dice [0.982, 0.9832, 0.9558] 
2023-11-30 10:05:24.917495: Epoch time: 66.27 s 
2023-11-30 10:05:26.201010:  
2023-11-30 10:05:26.206008: Epoch 111 
2023-11-30 10:05:26.210515: Current learning rate: 0.00177 
2023-11-30 10:06:32.817153: train_loss -0.9634 
2023-11-30 10:06:32.828182: val_loss -0.9264 
2023-11-30 10:06:32.854760: Pseudo dice [0.982, 0.9832, 0.956] 
2023-11-30 10:06:32.865278: Epoch time: 66.62 s 
2023-11-30 10:06:34.265404:  
2023-11-30 10:06:34.270864: Epoch 112 
2023-11-30 10:06:34.275861: Current learning rate: 0.00169 
2023-11-30 10:07:41.580660: train_loss -0.9634 
2023-11-30 10:07:41.688830: val_loss -0.923 
2023-11-30 10:07:41.753434: Pseudo dice [0.9822, 0.9825, 0.9546] 
2023-11-30 10:07:41.776466: Epoch time: 67.32 s 
2023-11-30 10:07:43.125381:  
2023-11-30 10:07:43.130710: Epoch 113 
2023-11-30 10:07:43.135600: Current learning rate: 0.0016 
2023-11-30 10:08:50.068800: train_loss -0.9637 
2023-11-30 10:08:50.090838: val_loss -0.9246 
2023-11-30 10:08:50.115026: Pseudo dice [0.9822, 0.9829, 0.9549] 
2023-11-30 10:08:50.186162: Epoch time: 66.94 s 
2023-11-30 10:08:51.586300:  
2023-11-30 10:08:51.592811: Epoch 114 
2023-11-30 10:08:51.597812: Current learning rate: 0.00152 
2023-11-30 10:10:00.825445: train_loss -0.964 
2023-11-30 10:10:00.845982: val_loss -0.9261 
2023-11-30 10:10:00.937657: Pseudo dice [0.982, 0.9834, 0.9558] 
2023-11-30 10:10:00.983736: Epoch time: 69.24 s 
2023-11-30 10:10:02.523091:  
2023-11-30 10:10:02.528601: Epoch 115 
2023-11-30 10:10:02.533605: Current learning rate: 0.00143 
2023-11-30 10:11:09.664353: train_loss -0.9641 
2023-11-30 10:11:09.709943: val_loss -0.9263 
2023-11-30 10:11:09.719458: Pseudo dice [0.9822, 0.9834, 0.9562] 
2023-11-30 10:11:09.729978: Epoch time: 67.14 s 
2023-11-30 10:11:11.161393:  
2023-11-30 10:11:11.167394: Epoch 116 
2023-11-30 10:11:11.171459: Current learning rate: 0.00135 
2023-11-30 10:12:19.910653: train_loss -0.9633 
2023-11-30 10:12:19.926184: val_loss -0.9228 
2023-11-30 10:12:19.941247: Pseudo dice [0.9824, 0.9825, 0.9551] 
2023-11-30 10:12:19.956775: Epoch time: 68.75 s 
2023-11-30 10:12:21.452636:  
2023-11-30 10:12:21.457306: Epoch 117 
2023-11-30 10:12:21.463187: Current learning rate: 0.00126 
2023-11-30 10:13:28.167591: train_loss -0.9639 
2023-11-30 10:13:28.179617: val_loss -0.926 
2023-11-30 10:13:28.193138: Pseudo dice [0.9823, 0.9829, 0.9555] 
2023-11-30 10:13:28.209179: Epoch time: 66.72 s 
2023-11-30 10:13:29.649879:  
2023-11-30 10:13:29.654877: Epoch 118 
2023-11-30 10:13:29.660385: Current learning rate: 0.00117 
2023-11-30 10:14:37.986605: train_loss -0.9639 
2023-11-30 10:14:38.516704: val_loss -0.922 
2023-11-30 10:14:38.525766: Pseudo dice [0.9816, 0.9827, 0.9552] 
2023-11-30 10:14:38.535284: Epoch time: 68.34 s 
2023-11-30 10:14:39.689594:  
2023-11-30 10:14:39.694589: Epoch 119 
2023-11-30 10:14:39.699878: Current learning rate: 0.00108 
2023-11-30 10:15:46.633864: train_loss -0.9647 
2023-11-30 10:15:46.644380: val_loss -0.925 
2023-11-30 10:15:46.653892: Pseudo dice [0.9821, 0.9832, 0.9562] 
2023-11-30 10:15:46.663410: Epoch time: 66.95 s 
2023-11-30 10:15:48.367653:  
2023-11-30 10:15:48.373652: Epoch 120 
2023-11-30 10:15:48.378203: Current learning rate: 0.00099 
2023-11-30 10:16:54.627620: train_loss -0.9643 
2023-11-30 10:16:54.657669: val_loss -0.9246 
2023-11-30 10:16:54.732284: Pseudo dice [0.9821, 0.9832, 0.9557] 
2023-11-30 10:16:54.771841: Epoch time: 66.26 s 
2023-11-30 10:16:56.152403:  
2023-11-30 10:16:56.157911: Epoch 121 
2023-11-30 10:16:56.163306: Current learning rate: 0.0009 
2023-11-30 10:18:01.332890: train_loss -0.9644 
2023-11-30 10:18:01.398015: val_loss -0.924 
2023-11-30 10:18:01.473154: Pseudo dice [0.9821, 0.983, 0.9553] 
2023-11-30 10:18:01.489187: Epoch time: 65.18 s 
2023-11-30 10:18:02.695215:  
2023-11-30 10:18:02.701726: Epoch 122 
2023-11-30 10:18:02.707232: Current learning rate: 0.00081 
2023-11-30 10:19:07.687795: train_loss -0.964 
2023-11-30 10:19:07.759912: val_loss -0.9242 
2023-11-30 10:19:07.777944: Pseudo dice [0.9823, 0.9828, 0.9558] 
2023-11-30 10:19:07.794474: Epoch time: 64.99 s 
2023-11-30 10:19:09.178695:  
2023-11-30 10:19:09.184374: Epoch 123 
2023-11-30 10:19:09.189662: Current learning rate: 0.00072 
2023-11-30 10:20:15.608252: train_loss -0.9646 
2023-11-30 10:20:15.619769: val_loss -0.9241 
2023-11-30 10:20:15.633303: Pseudo dice [0.9822, 0.9832, 0.9558] 
2023-11-30 10:20:15.645818: Epoch time: 66.43 s 
2023-11-30 10:20:17.113905:  
2023-11-30 10:20:17.119496: Epoch 124 
2023-11-30 10:20:17.124462: Current learning rate: 0.00063 
2023-11-30 10:21:24.079541: train_loss -0.965 
2023-11-30 10:21:24.094055: val_loss -0.9231 
2023-11-30 10:21:24.110097: Pseudo dice [0.9825, 0.9827, 0.9549] 
2023-11-30 10:21:24.122620: Epoch time: 66.97 s 
2023-11-30 10:21:25.748885:  
2023-11-30 10:21:25.753884: Epoch 125 
2023-11-30 10:21:25.758397: Current learning rate: 0.00053 
2023-11-30 10:22:32.006906: train_loss -0.9645 
2023-11-30 10:22:32.021437: val_loss -0.9229 
2023-11-30 10:22:32.031960: Pseudo dice [0.9828, 0.9831, 0.9549] 
2023-11-30 10:22:32.042481: Epoch time: 66.26 s 
2023-11-30 10:22:33.539726:  
2023-11-30 10:22:33.545182: Epoch 126 
2023-11-30 10:22:33.549491: Current learning rate: 0.00044 
2023-11-30 10:23:38.731534: train_loss -0.9649 
2023-11-30 10:23:38.768593: val_loss -0.9204 
2023-11-30 10:23:38.814170: Pseudo dice [0.982, 0.9825, 0.9537] 
2023-11-30 10:23:38.823688: Epoch time: 65.19 s 
2023-11-30 10:23:40.243109:  
2023-11-30 10:23:40.248617: Epoch 127 
2023-11-30 10:23:40.253617: Current learning rate: 0.00034 
2023-11-30 10:24:45.315852: train_loss -0.965 
2023-11-30 10:24:45.331375: val_loss -0.9251 
2023-11-30 10:24:45.341891: Pseudo dice [0.9819, 0.9832, 0.9564] 
2023-11-30 10:24:45.351412: Epoch time: 65.07 s 
2023-11-30 10:24:46.805470:  
2023-11-30 10:24:46.811083: Epoch 128 
2023-11-30 10:24:46.815130: Current learning rate: 0.00023 
2023-11-30 10:25:51.519233: train_loss -0.9649 
2023-11-30 10:25:51.531764: val_loss -0.9249 
2023-11-30 10:25:51.542280: Pseudo dice [0.9822, 0.9833, 0.9557] 
2023-11-30 10:25:51.551792: Epoch time: 64.72 s 
2023-11-30 10:25:53.030529:  
2023-11-30 10:25:53.036036: Epoch 129 
2023-11-30 10:25:53.041037: Current learning rate: 0.00013 
2023-11-30 10:26:57.778313: train_loss -0.9651 
2023-11-30 10:26:57.788826: val_loss -0.9243 
2023-11-30 10:26:57.798339: Pseudo dice [0.9821, 0.9833, 0.9562] 
2023-11-30 10:26:57.810859: Epoch time: 64.75 s 
2023-11-30 10:26:59.932521: Training done. 
2023-11-30 10:27:00.002788: Using splits from existing split file: C:/Users/qwe14/0.code/nnUNet/nnUNet_preprocessed\Dataset121_MR\splits_final.json 
2023-11-30 10:27:00.024863: The split file contains 5 splits. 
2023-11-30 10:27:00.032876: Desired fold for training: 5 
2023-11-30 10:27:00.040389: INFO: You requested fold 5 for training but splits contain only 5 folds. I am now creating a random (but seeded) 80:20 split! 
2023-11-30 10:27:00.055418: This random 80:20 split has 776 training and 194 validation cases. 
2023-11-30 10:27:00.067940: predicting img-00009 
2023-11-30 10:27:00.742001: predicting img-00012 
2023-11-30 10:27:00.809088: predicting img-00014 
2023-11-30 10:27:00.871174: predicting img-00020 
2023-11-30 10:27:00.935891: predicting img-00025 
2023-11-30 10:27:00.995129: predicting img-00036 
2023-11-30 10:27:01.051231: predicting img-00038 
2023-11-30 10:27:01.117428: predicting img-00042 
2023-11-30 10:27:01.189065: predicting img-00044 
2023-11-30 10:27:01.253488: predicting img-00065 
2023-11-30 10:27:05.759834: predicting img-00067 
2023-11-30 10:27:06.027213: predicting img-00068 
2023-11-30 10:27:06.185435: predicting img-00075 
2023-11-30 10:27:06.284572: predicting img-00083 
2023-11-30 10:27:06.383209: predicting img-00091 
2023-11-30 10:27:06.489344: predicting img-00093 
2023-11-30 10:27:06.591644: predicting img-00095 
2023-11-30 10:27:06.668252: predicting img-00106 
2023-11-30 10:27:06.749355: predicting img-00113 
2023-11-30 10:27:06.834469: predicting img-00120 
2023-11-30 10:27:06.910600: predicting img-00129 
2023-11-30 10:27:06.987685: predicting img-00130 
2023-11-30 10:27:07.066435: predicting img-00133 
2023-11-30 10:27:07.143019: predicting img-00134 
2023-11-30 10:27:07.220248: predicting img-00137 
2023-11-30 10:27:07.298337: predicting img-00138 
2023-11-30 10:27:07.376438: predicting img-00148 
2023-11-30 10:27:07.453587: predicting img-00153 
2023-11-30 10:27:07.533307: predicting img-00163 
2023-11-30 10:27:07.608902: predicting img-00167 
2023-11-30 10:27:07.686351: predicting img-00169 
2023-11-30 10:27:07.763442: predicting img-00178 
2023-11-30 10:27:07.843037: predicting img-00186 
2023-11-30 10:27:07.922639: predicting img-00187 
2023-11-30 10:27:08.000103: predicting img-00188 
2023-11-30 10:27:08.079698: predicting img-00198 
2023-11-30 10:27:08.159308: predicting img-00207 
2023-11-30 10:27:08.235395: predicting img-00210 
2023-11-30 10:27:08.311976: predicting img-00220 
2023-11-30 10:27:08.390059: predicting img-00236 
2023-11-30 10:27:08.469167: predicting img-00238 
2023-11-30 10:27:08.546768: predicting img-00245 
2023-11-30 10:27:08.627358: predicting img-00246 
2023-11-30 10:27:08.704722: predicting img-00248 
2023-11-30 10:27:08.782817: predicting img-00249 
2023-11-30 10:27:08.864415: predicting img-00254 
2023-11-30 10:27:08.943017: predicting img-00261 
2023-11-30 10:27:09.020621: predicting img-00279 
2023-11-30 10:27:09.096714: predicting img-00281 
2023-11-30 10:27:09.172815: predicting img-00288 
2023-11-30 10:27:09.249410: predicting img-00296 
2023-11-30 10:27:09.325000: predicting img-00300 
2023-11-30 10:27:09.403599: predicting img-00309 
2023-11-30 10:27:09.482203: predicting img-00311 
2023-11-30 10:27:09.561731: predicting img-00316 
2023-11-30 10:27:09.638325: predicting img-00328 
2023-11-30 10:27:09.714378: predicting img-00336 
2023-11-30 10:27:09.791964: predicting img-00341 
2023-11-30 10:27:09.871559: predicting img-00346 
2023-11-30 10:27:09.950160: predicting img-00355 
2023-11-30 10:27:10.026762: predicting img-00358 
2023-11-30 10:27:10.105714: predicting img-00371 
2023-11-30 10:27:10.181534: predicting img-00383 
2023-11-30 10:27:10.258868: predicting img-00388 
2023-11-30 10:27:10.336463: predicting img-00393 
2023-11-30 10:27:10.413553: predicting img-00398 
2023-11-30 10:27:10.492647: predicting img-00401 
2023-11-30 10:27:10.574757: predicting img-00403 
2023-11-30 10:27:10.657379: predicting img-00405 
2023-11-30 10:27:10.740557: predicting img-00407 
2023-11-30 10:27:10.817149: predicting img-00414 
2023-11-30 10:27:10.894232: predicting img-00418 
2023-11-30 10:27:10.974830: predicting img-00424 
2023-11-30 10:27:11.056445: predicting img-00428 
2023-11-30 10:27:11.136303: predicting img-00436 
2023-11-30 10:27:11.214392: predicting img-00437 
2023-11-30 10:27:11.290481: predicting img-00439 
2023-11-30 10:27:11.366793: predicting img-00450 
2023-11-30 10:27:11.443204: predicting img-00463 
2023-11-30 10:27:11.521802: predicting img-00468 
2023-11-30 10:27:11.605305: predicting img-00471 
2023-11-30 10:27:11.685428: predicting img-00475 
2023-11-30 10:27:11.761879: predicting img-00477 
2023-11-30 10:27:11.838474: predicting img-00483 
2023-11-30 10:27:11.915062: predicting img-00484 
2023-11-30 10:27:11.997177: predicting img-00485 
2023-11-30 10:27:12.078319: predicting img-00493 
2023-11-30 10:27:12.155927: predicting img-00495 
2023-11-30 10:27:12.233017: predicting img-00497 
2023-11-30 10:27:12.311622: predicting img-00509 
2023-11-30 10:27:12.389704: predicting img-00511 
2023-11-30 10:27:12.465299: predicting img-00517 
2023-11-30 10:27:12.544451: predicting img-00518 
2023-11-30 10:27:12.621050: predicting img-00523 
2023-11-30 10:27:12.699139: predicting img-00524 
2023-11-30 10:27:12.777754: predicting img-00530 
2023-11-30 10:27:12.856349: predicting img-00537 
2023-11-30 10:27:12.932437: predicting img-00538 
2023-11-30 10:27:13.009187: predicting img-00546 
2023-11-30 10:27:13.086785: predicting img-00548 
2023-11-30 10:27:13.165388: predicting img-00556 
2023-11-30 10:27:13.244988: predicting img-00573 
2023-11-30 10:27:13.325598: predicting img-00577 
2023-11-30 10:27:13.401975: predicting img-00580 
2023-11-30 10:27:13.478589: predicting img-00608 
2023-11-30 10:27:13.553677: predicting img-00614 
2023-11-30 10:27:13.630279: predicting img-00615 
2023-11-30 10:27:13.704365: predicting img-00620 
2023-11-30 10:27:13.783478: predicting img-00630 
2023-11-30 10:27:13.862079: predicting img-00633 
2023-11-30 10:27:13.939211: predicting img-00651 
2023-11-30 10:27:14.014676: predicting img-00656 
2023-11-30 10:27:14.093273: predicting img-00658 
2023-11-30 10:27:14.214322: predicting img-00664 
2023-11-30 10:27:14.286840: predicting img-00665 
2023-11-30 10:27:14.365621: predicting img-00675 
2023-11-30 10:27:14.447230: predicting img-00676 
2023-11-30 10:27:14.523314: predicting img-00680 
2023-11-30 10:27:14.601406: predicting img-00681 
2023-11-30 10:27:14.683055: predicting img-00683 
2023-11-30 10:27:14.762375: predicting img-00688 
2023-11-30 10:27:14.841980: predicting img-00694 
2023-11-30 10:27:14.922393: predicting img-00696 
2023-11-30 10:27:15.000492: predicting img-00697 
2023-11-30 10:27:15.082100: predicting img-00698 
2023-11-30 10:27:15.176321: predicting img-00700 
2023-11-30 10:27:15.318674: predicting img-00706 
2023-11-30 10:27:15.431922: predicting img-00707 
2023-11-30 10:27:15.568162: predicting img-00712 
2023-11-30 10:27:15.650580: predicting img-00715 
2023-11-30 10:27:15.729708: predicting img-00716 
2023-11-30 10:27:15.810821: predicting img-00718 
2023-11-30 10:27:15.894417: predicting img-00730 
2023-11-30 10:27:15.980143: predicting img-00744 
2023-11-30 10:27:16.064302: predicting img-00746 
2023-11-30 10:27:16.163519: predicting img-00750 
2023-11-30 10:27:16.256734: predicting img-00765 
2023-11-30 10:27:16.347446: predicting img-00767 
2023-11-30 10:27:16.432617: predicting img-00769 
2023-11-30 10:27:16.516816: predicting img-00770 
2023-11-30 10:27:16.600981: predicting img-00778 
2023-11-30 10:27:16.686174: predicting img-00780 
2023-11-30 10:27:16.771422: predicting img-00786 
2023-11-30 10:27:16.861110: predicting img-00787 
2023-11-30 10:27:16.947325: predicting img-00788 
2023-11-30 10:27:17.036511: predicting img-00806 
2023-11-30 10:27:17.122182: predicting img-00809 
2023-11-30 10:27:17.204854: predicting img-00817 
2023-11-30 10:27:17.336220: predicting img-00819 
2023-11-30 10:27:17.414305: predicting img-00825 
2023-11-30 10:27:17.490206: predicting img-00828 
2023-11-30 10:27:17.571820: predicting img-00829 
2023-11-30 10:27:17.649428: predicting img-00830 
2023-11-30 10:27:17.726624: predicting img-00835 
2023-11-30 10:27:17.804703: predicting img-00838 
2023-11-30 10:27:17.882489: predicting img-00849 
2023-11-30 10:27:17.959085: predicting img-00850 
2023-11-30 10:27:18.038691: predicting img-00854 
2023-11-30 10:27:18.116847: predicting img-00855 
2023-11-30 10:27:18.199460: predicting img-00859 
2023-11-30 10:27:18.276052: predicting img-00867 
2023-11-30 10:27:18.352886: predicting img-00895 
2023-11-30 10:27:18.432483: predicting img-00900 
2023-11-30 10:27:18.512577: predicting img-00901 
2023-11-30 10:27:18.591176: predicting img-00902 
2023-11-30 10:27:18.670434: predicting img-00905 
2023-11-30 10:27:18.748037: predicting img-00911 
2023-11-30 10:27:18.826639: predicting img-00912 
2023-11-30 10:27:18.904721: predicting img-00915 
2023-11-30 10:27:18.981544: predicting img-00917 
2023-11-30 10:27:19.063146: predicting img-00919 
2023-11-30 10:27:19.141753: predicting img-00920 
2023-11-30 10:27:19.217848: predicting img-00921 
2023-11-30 10:27:19.295377: predicting img-00931 
2023-11-30 10:27:19.372468: predicting img-00934 
2023-11-30 10:27:19.450595: predicting img-00935 
2023-11-30 10:27:19.527193: predicting img-00941 
2023-11-30 10:27:19.602773: predicting img-00956 
2023-11-30 10:27:19.681373: predicting img-00961 
2023-11-30 10:27:19.761638: predicting img-00966 
2023-11-30 10:27:19.838170: predicting img-00981 
2023-11-30 10:27:19.916265: predicting img-00982 
2023-11-30 10:27:19.992350: predicting img-00990 
2023-11-30 10:27:20.069949: predicting img-00995 
2023-11-30 10:27:20.148548: predicting img-01002 
2023-11-30 10:27:20.232191: predicting img-01020 
2023-11-30 10:27:20.308801: predicting img-01022 
2023-11-30 10:27:20.386166: predicting img-01026 
2023-11-30 10:27:20.463253: predicting img-01032 
2023-11-30 10:27:20.542897: predicting img-01036 
2023-11-30 10:27:20.621001: predicting img-01037 
2023-11-30 10:27:20.704985: predicting img-01038 
2023-11-30 10:27:20.792640: predicting img-01051 
2023-11-30 10:27:20.875863: predicting img-01075 
2023-11-30 10:27:26.120965: Validation complete 
2023-11-30 10:27:26.125984: Mean Validation Dice:  0.973486131135 
